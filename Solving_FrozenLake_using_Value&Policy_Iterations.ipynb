{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solving FrozenLake using Value&Policy Iterations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AvivSham/Reinforcement-Learning/blob/master/Solving_FrozenLake_using_Value&Policy_Iterations.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "hqILDZjOQf_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Solving FrozenLake Env using Value and Policy Iteration Algorithms\n",
        "####You can import this code in order to solve other Envs in OpenAI gym - do not forget to change the name of the environment!!!\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "H4pzahOXq10P",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Installing OpenAI gym & importing dependencies\n",
        "!pip install gym\n",
        "from time import time\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRXUsvH6rpnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Evaluate the policy efficiency\n",
        "def runPolicy(env, policy):\n",
        "  \n",
        "  # Initializing\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  totalReward = 0\n",
        "  \n",
        "  while not done:\n",
        "    state, reward, done, _ = env.step(policy[state])\n",
        "    totalReward += reward\n",
        "  \n",
        "  return totalReward\n",
        "\n",
        "def evaluatePolicy(env, policy, iterations):\n",
        "  totalRewards = 0\n",
        "  for i in range(iterations):\n",
        "    totalRewards += runPolicy(env, policy)\n",
        "  return totalRewards / iterations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmYSydQIt3b5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Common functions for value iteration and policy iteration - calculating both policies and state values\n",
        "eps = 1e-10\n",
        "\n",
        "def constructGridPolicy(env, values, gamma):\n",
        "  policy = np.zeros(env.env.nS)\n",
        "  for s in range(env.env.nS):\n",
        "    returns = [sum(p * (r + gamma * values[ns])\n",
        "                  for p, ns, r, _ in env.env.P[s][a])\n",
        "               for a in range (env.env.nA)\n",
        "    ]\n",
        "    policy[s] = np.argmax(returns)\n",
        "  \n",
        "  return policy\n",
        "    \n",
        "\n",
        "def computeStateValues(env, gamma, policy = None, selectBest = False):\n",
        "  if policy is None and not selectBest:\n",
        "    raise 'When running computeStateValues specifying policy or selectBest = True is necessary'\n",
        "  if policy is not None and selectBest:\n",
        "    raise 'You cannot use policy and selectBest at the same time'\n",
        "  \n",
        "  values = np.zeros(env.env.nS)\n",
        "  while True:\n",
        "    nextValues = values.copy()\n",
        "    for s in range (env.env.nS):\n",
        "      if policy is not None:\n",
        "        action = policy[s]\n",
        "        # Bellman equation\n",
        "        nextValues[s] = sum(p * (r + gamma * values[ns]) for p, ns, r, _ in env.env.P[s][action])\n",
        "      else:\n",
        "        # Bellman equation\n",
        "        nextValues[s] = max(sum(p * (r + gamma * values[ns])\n",
        "                               for p, ns, r, _ in env.env.P[s][a])\n",
        "                            for a in range (env.env.nA)\n",
        "                           )\n",
        "    diff = np.fabs (nextValues - values).sum()\n",
        "    values = nextValues\n",
        "    if diff <= eps:\n",
        "      break\n",
        "     \n",
        "  return values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYs3vx9VwUZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Value-iteration algorithm implementation\n",
        "def valueIteration (env, gamma):\n",
        "  stateValues = computeStateValues(env, gamma, selectBest=True)\n",
        "  policy = constructGridPolicy(env, stateValues, gamma)\n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLVsqZWVwUbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Value-iteration algorithm implementation\n",
        "\n",
        "# Initializing the policy grid\n",
        "def initPolicy(env):\n",
        "  return np.random.choice(env.env.nA, size=(env.env.nS))\n",
        "\n",
        "# Policy update every iteration\n",
        "def policyIteration(env, gamma):\n",
        "  policy = initPolicy(env)\n",
        "  while True:\n",
        "    stateValues = computeStateValues(env, gamma, selectBest = True)\n",
        "    nextPolicy = constructGridPolicy (env, stateValues, gamma)\n",
        "    if np.all(policy == nextPolicy):\n",
        "      break\n",
        "      \n",
        "    policy = nextPolicy\n",
        "    \n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AaqhrMaU3fe6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Environment solving function\n",
        "def solveEnv (env, methods, envName):\n",
        "  print(f'Solving environment {envName}')\n",
        "  for method in methods:\n",
        "    name, f, gamma = method\n",
        "    startTime = time()\n",
        "    policy = f(env, gamma)\n",
        "    endTime = time()\n",
        "    print(f'It took {endTime - startTime} to train the policy with {name} algorithm , Gamma = {gamma}')\n",
        "    \n",
        "    \n",
        "    policyScore = evaluatePolicy(env, policy, evaluateIterations)\n",
        "    print(f'The averaged policy reward is: {policyScore}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Nj_ZVUPwUe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "70495e90-1839-4356-a062-f366056d2668"
      },
      "cell_type": "code",
      "source": [
        "#@title Main code - solving FrozenLake 4x4\n",
        "\n",
        "# You can import the code and use the functions without the main code part\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "  \n",
        "  evaluateIterations = 1000\n",
        "  methods = [\n",
        "  ('Value Iteration', valueIteration, 0.9),\n",
        "  ('Policy Iteration', policyIteration, 0.9),\n",
        "  ('Value Iteration', valueIteration, 0.98),\n",
        "  ('Policy Iteration', policyIteration, 0.98),\n",
        "  ('Value Iteration', valueIteration, 1),\n",
        "  ('Policy Iteration', policyIteration, 1),\n",
        "]\n",
        "\n",
        "frozenLake4x4 = gym.make('FrozenLake-v0')\n",
        "solveEnv(frozenLake4x4, methods, 'Frozen Lake 4x4')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment Frozen Lake 4x4\n",
            "It took 0.025897502899169922 to train the policy with Value Iteration algorithm , Gamma = 0.9\n",
            "The averaged policy reward is: 0.749\n",
            "It took 0.05134749412536621 to train the policy with Policy Iteration algorithm , Gamma = 0.9\n",
            "The averaged policy reward is: 0.754\n",
            "It took 0.07741785049438477 to train the policy with Value Iteration algorithm , Gamma = 0.98\n",
            "The averaged policy reward is: 0.741\n",
            "It took 0.14547014236450195 to train the policy with Policy Iteration algorithm , Gamma = 0.98\n",
            "The averaged policy reward is: 0.749\n",
            "It took 0.15343689918518066 to train the policy with Value Iteration algorithm , Gamma = 1\n",
            "The averaged policy reward is: 0.723\n",
            "It took 0.3515298366546631 to train the policy with Policy Iteration algorithm , Gamma = 1\n",
            "The averaged policy reward is: 0.741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TcfzIPEP9lkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Compare to FrozenLake 8x8\n",
        "\n",
        "frozenLake8x8 = gym.make('FrozenLake8x8-v0')\n",
        "solveEnv(FrozenLake8x8, methods, 'Frozen Lake 8x8')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}