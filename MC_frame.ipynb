{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MC_frame.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AvivSham/Reinforcement-Learning/blob/master/MC_frame.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yxWzsySY-h9I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo framework for traning on-policy methods"
      ]
    },
    {
      "metadata": {
        "id": "YSlq-jSY-ZJR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "General purpose Monte Carlo model for training on-policy methods.\n",
        "\"\"\"\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "class FiniteMCModel:\n",
        "    def __init__(self, state_space, action_space, gamma=1.0, epsilon=0.1):\n",
        "        \"\"\"MCModel takes in state_space and action_space (finite) \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        state_space: int OR list[observation], where observation is any hashable type from env's obs.\n",
        "        action_space: int OR list[action], where action is any hashable type from env's actions.\n",
        "        gamma: float, discounting factor.\n",
        "        epsilon: float, epsilon-greedy parameter.\n",
        "        \n",
        "        If the parameter is an int, then we generate a list, and otherwise we generate a dictionary.\n",
        "        >>> m = FiniteMCModel(2,3,epsilon=0)\n",
        "        >>> m.Q\n",
        "        [[0, 0, 0], [0, 0, 0]]\n",
        "        >>> m.Q[0][1] = 1\n",
        "        >>> m.Q\n",
        "        [[0, 1, 0], [0, 0, 0]]\n",
        "        >>> m.pi(1, 0)\n",
        "        1\n",
        "        >>> m.pi(1, 1)\n",
        "        0\n",
        "        >>> d = m.generate_returns([(0,0,0), (0,1,1), (1,0,1)])\n",
        "        >>> assert(d == {(1, 0): 1, (0, 1): 2, (0, 0): 2})\n",
        "        >>> m.choose_action(m.pi, 1)\n",
        "        0\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = None\n",
        "        if isinstance(action_space, int):\n",
        "            self.action_space = np.arange(action_space)\n",
        "            actions = [0]*action_space\n",
        "            # Action representation\n",
        "            self._act_rep = \"list\"\n",
        "        else:\n",
        "            self.action_space = action_space\n",
        "            actions = {k:0 for k in action_space}\n",
        "            self._act_rep = \"dict\"\n",
        "        if isinstance(state_space, int):\n",
        "            self.state_space = np.arange(state_space)\n",
        "            self.Q = [deepcopy(actions) for _ in range(state_space)]\n",
        "        else:\n",
        "            self.state_space = state_space\n",
        "            self.Q = {k:deepcopy(actions) for k in state_space}\n",
        "            \n",
        "        # Frequency of state/action.\n",
        "        self.Ql = deepcopy(self.Q)\n",
        "\n",
        "        \n",
        "    def pi(self, action, state):\n",
        "        \"\"\"pi(a,s,A,V) := pi(a|s)\n",
        "        We take the argmax_a of Q(s,a).\n",
        "        q[s] = [q(s,0), q(s,1), ...]\n",
        "        \"\"\"\n",
        "        if self._act_rep == \"list\":\n",
        "            if action == np.argmax(self.Q[state]):\n",
        "                return 1\n",
        "            return 0\n",
        "        elif self._act_rep == \"dict\":\n",
        "            if action == max(self.Q[state], key=self.Q[state].get):\n",
        "                return 1\n",
        "            return 0\n",
        "    \n",
        "    \n",
        "    def b(self, action, state):\n",
        "        \"\"\"b(a,s,A) := b(a|s) \n",
        "        Sometimes you can only use a subset of the action space\n",
        "        given the state.\n",
        "\n",
        "        Randomly selects an action from a uniform distribution.\n",
        "        \"\"\"\n",
        "        return self.epsilon/len(self.action_space) + (1-self.epsilon) * self.pi(action, state)\n",
        "\n",
        "    \n",
        "    def generate_returns(self, ep):\n",
        "        \"\"\"Backup on returns per time period in an epoch\n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        ep: [(observation, action, reward)], an episode trajectory in chronological order.\n",
        "        \"\"\"\n",
        "        G = {} # return on state\n",
        "        C = 0 # cumulative reward\n",
        "        for tpl in reversed(ep):\n",
        "            observation, action, reward = tpl\n",
        "            G[(observation, action)] = C = reward + self.gamma*C\n",
        "        return G\n",
        "    \n",
        "    \n",
        "    def choose_action(self, policy, state):\n",
        "        \"\"\"Uses specified policy to select an action randomly given the state.\n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        policy: function, can be self.pi, or self.b, or another custom policy.\n",
        "        state: observation of the environment.\n",
        "        \"\"\"\n",
        "        probs = [policy(a, state) for a in self.action_space]\n",
        "        return np.random.choice(self.action_space, p=probs)\n",
        "\n",
        "    \n",
        "    def update_Q(self, ep):\n",
        "        \"\"\"Performs a action-value update.\n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        ep: [(observation, action, reward)], an episode trajectory in chronological order.\n",
        "        \"\"\"\n",
        "        # Generate returns, return ratio\n",
        "        G = self.generate_returns(ep)\n",
        "        for s in G:\n",
        "            state, action = s\n",
        "            q = self.Q[state][action]\n",
        "            self.Ql[state][action] += 1\n",
        "            N = self.Ql[state][action]\n",
        "            self.Q[state][action] = q * N/(N+1) + G[s]/(N+1)\n",
        "    \n",
        "    def score(self, env, policy, n_samples=1000):\n",
        "        \"\"\"Evaluates a specific policy with regards to the env.\n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        env: an openai gym env, or anything that follows the api.\n",
        "        policy: a function, could be self.pi, self.b, etc.\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        for _ in range(n_samples):\n",
        "            observation = env.reset()\n",
        "            cum_rewards = 0\n",
        "            while True:\n",
        "                action = self.choose_action(policy, observation)\n",
        "                observation, reward, done, _ = env.step(action)\n",
        "                cum_rewards += reward\n",
        "                if done:\n",
        "                    rewards.append(cum_rewards)\n",
        "                    break\n",
        "        return np.mean(rewards)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}